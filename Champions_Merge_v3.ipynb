{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Commercial EN SE Champions Mashup Script\n",
    "#### Created by Jeff Alexander (jefalexa@cisco.com)\n",
    "#### Updated on 2/28/19 by Jeff Alexander (jefalexa@cisco.com)\n",
    "\n",
    "#### Description\n",
    "This script is used to maintain an accurate list of the designated EN Champions for reference and distribution.  It combines Directory Org structure, SAGE technical expertise and SEVT membership information downloaded by the user into a comprehensive and up to date list.  It then updates the published SmartSheet for distribution.  \n",
    "\n",
    "#### SmartSheet Info\n",
    "- Name:  \"EN Champions\"\n",
    "- URL:  https://app.smartsheet.com/sheets/JFc989gxfXM7hF4vhP2f2QpQQ28XgwMq6Ch6xhJ1?view=grid\n",
    "- SheetID:  \"809447254714244\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Environment\n",
    "#### If not already there, add your SmartSheet API Key to the \"api_keys.py\" file\n",
    "#### Ensure that this script is running in the provided Virtual Environment container to avoid module dependancy issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Pull directory dump for “Who is my Champion?” correlation\n",
    "https://labtools.cisco.com/general/orgchart.php?tops=lydohert&photos=none\n",
    "\n",
    "Click \"Raw Data\"\n",
    "\n",
    "### SAGE data\n",
    "https://wwss.cisco.com/#/site/WWSE/views/Sage-BasicDashboards_0/DetailedExpertiseDash/jefalexa@cisco.com/AreaENChampions?:iid=4\n",
    "\n",
    "Click on the main table to select it, then \"Download\" >> \"Download all rows as a text file\"\n",
    "        \n",
    "### SEVT membership / attendance\n",
    "https://wwss.cisco.com/#/site/WWSE/views/Sage-BasicDashboards_0/VTMembersDash/jefalexa@cisco.com/USCENSEVTMember?:iid=6\n",
    "\n",
    "Click on the main table to select it, then \"Download\" >> \"Download all rows as a text file\"\n",
    "\n",
    "Archive the old files and move the new files to the local working directory\n",
    "- searchresult.csv = Directory Info\n",
    "- Detailed_Expertise_data.csv = SAGE Data\n",
    "- VT_Members_data.csv = SEVT Membership\n",
    "\n",
    "\n",
    "\n",
    "## Run the Code\n",
    "Once you have verified that the new files are copied locally, run the code by selecting \"Cell\" --> \"Run All\" from the top menu bar.  It will run each of the cells below in sequence, logging progress as it goes.  The cells are broken up into logical blocks to make troubleshooting and updates eaiser.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re, sys, time, csv\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import numpy as np\n",
    "import datetime\n",
    "import smartsheet\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import api_keys\n",
    "\n",
    "print(\"STARTING - {0}\".format(datetime.datetime.now()))\n",
    "\n",
    "# Define imput/output files\n",
    "input_file1 = \"Detailed_Expertise_data.csv\"\n",
    "input_file2 = \"VT_Members_data.csv\"\n",
    "input_file3 = \"searchresult.csv\"\n",
    "input_file4 = \"dept_mapping.csv\"\n",
    "output_file = \"default_output_file.xlsx\"\n",
    "\n",
    "# Import CSV files as Pandas Data Frames\n",
    "INPUT_SAGE1 = pd.read_csv(input_file1, quotechar='\"', low_memory=False, sep='\\t', encoding='utf-16')\n",
    "INPUT_SAGE2 = pd.read_csv(input_file2, quotechar='\"', low_memory=False, sep='\\t', encoding='utf-16')\n",
    "INPUT_DIR1 = pd.read_csv(input_file3, quotechar='\"', low_memory=False, encoding='utf-8-sig')\n",
    "INPUT_DIR2 = pd.read_csv(input_file4, quotechar='\"', low_memory=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"Initialize SmartSheet Client...\")\n",
    "# Initialize client\n",
    "ss = smartsheet.Smartsheet(api_keys.ss_access_token)\n",
    "# Make sure we don't miss any error\n",
    "ss.errors_as_exceptions(True)\n",
    "\n",
    "# Log all calls\n",
    "logging.basicConfig(filename='rwsheet.log', level=logging.INFO)\n",
    "    \n",
    "\n",
    "# Define Functions\n",
    "print(\"Initialize Functions...\")\n",
    "def get_column_names(sheet_id):\n",
    "    \"\"\"Returns a dict of the column names and their corresponding ID's in the working Sheet (sheet_id)\"\"\"\n",
    "    column_names = {}\n",
    "    column_list = ss.Sheets.get_columns(sheet_id).to_dict()\n",
    "    for i in column_list['data']:\n",
    "        column_names[i['title']] = i['id']\n",
    "    return(column_names)\n",
    "\n",
    "def get_row_search(sheet_id, search_string, search_column):\n",
    "    \"\"\"Searches the working sheet (sheet_id) for a term (search_string), \n",
    "    then verifies that the term is in the specific colum you are referencing (search_column)\n",
    "    returns a list of column ID's that match both\"\"\"\n",
    "    sr = ss.Sheets.search_sheet(sheet_id, search_string).to_dict()['results']\n",
    "    sr_list = []\n",
    "    for x in sr:\n",
    "        my_row_d = ss.Sheets.get_row(sheet_id, x['objectId']).to_dict()\n",
    "        for y in my_row_d['cells']:\n",
    "            if (y['columnId'] == column_names[search_column]):\n",
    "                if (y['value'] == search_string):\n",
    "                    sr_list.append(x['objectId'])\n",
    "    return(sr_list)\n",
    "\n",
    "\n",
    "def update_cell(sheet_id, column_id, row_id, new_value):\n",
    "    \"\"\"Updates the value of a give cell (column_id, row_id) with a new value (new_value)\"\"\"\n",
    "    # Build new cell value\n",
    "    new_cell = ss.models.Cell()\n",
    "    new_cell.column_id = column_id\n",
    "    new_cell.value = str(new_value)\n",
    "    new_cell.strict = False\n",
    "\n",
    "    # Build the row to update\n",
    "    new_row = ss.models.Row()\n",
    "    new_row.id = row_id\n",
    "    new_row.cells.append(new_cell)\n",
    "\n",
    "    # Update row\n",
    "    updated_row = ss.Sheets.update_rows(sheet_id, [new_row])\n",
    "    \n",
    "\n",
    "def update_cells(sheet_id, row_search, column_search, column_name, new_value):\n",
    "    \"\"\"Gets a list of rows (row_list) by calling get_row_search with the search term (row_search) and column name to search in (column_search)\n",
    "    gets the column_id from the column_name and updates that cell with the new_value in each of the rows matched.\n",
    "    Returns the number of rows matched by the search and the number updated as a string.  \"\"\"\n",
    "    row_list = get_row_search(sheet_id, row_search, column_search)\n",
    "    column_id = column_names[column_name]\n",
    "    row_count = len(row_list)\n",
    "    update_count = 0\n",
    "    for r in row_list:\n",
    "        update_cell(sheet_id, column_id, r, new_value)\n",
    "        update_count += 1\n",
    "    return(\"Updated {0} of {1} rows\".format(update_count, row_count))\n",
    "\n",
    "def update_cells_by_dict(update_dict):\n",
    "    progress_total = len(update_dict)\n",
    "    progress_count = 1 \n",
    "    for y in update_dict:\n",
    "        print(\"Processing {0} of {1} UID's\".format(progress_count, progress_total))\n",
    "        progress_count += 1 \n",
    "        L1_id = y\n",
    "        L1_value = update_dict[L1_id]\n",
    "        for L2_id in L1_value:\n",
    "            print(update_cells(sheet_id, L1_id, \"uid\", L2_id, L1_value[L2_id]))\n",
    "   \n",
    "def set_expertise(x):\n",
    "    Exp_Found = re.compile(\".*Foundational.*\",re.IGNORECASE)\n",
    "    Exp_Adv = re.compile(\".*Advanced.*\",re.IGNORECASE)\n",
    "    Exp_ID = re.compile(\".*In-Depth.*\",re.IGNORECASE) \n",
    "    try:\n",
    "        if Exp_Found.match(x):\n",
    "            return(1)\n",
    "        elif Exp_Adv.match(x):\n",
    "            return(2)\n",
    "        elif Exp_ID.match(x):\n",
    "            return(3)\n",
    "        else:\n",
    "            return(0)\n",
    "    except:\n",
    "        return(0)\n",
    "\n",
    "def set_email(x):\n",
    "    try:\n",
    "        if len(x) > 2:\n",
    "            return(\"{0}@cisco.com\".format(x))\n",
    "        else:\n",
    "            return(\"Unknown\")\n",
    "    except:\n",
    "        return(\"Unknown\")\n",
    "    \n",
    "\n",
    "def split_email(s1):\n",
    "    try:\n",
    "        if len(s1) > 2:\n",
    "            s2 = s1.split('@')\n",
    "            return(s2[0])\n",
    "        else:\n",
    "            return(\"Unknown\")\n",
    "    except:\n",
    "        return(\"Unknown\")\n",
    "\n",
    "    \n",
    "def set_dir(x):\n",
    "    try:\n",
    "        if len(x) > 2:\n",
    "            return(\"http://directory.cisco.com/dir/reports/{0}\".format(x))\n",
    "        else:\n",
    "            return(\"Unknown\")\n",
    "    except:\n",
    "        return(\"Unknown\")\n",
    "\n",
    "\n",
    "def max_exp_rtr(x):\n",
    "    y = \"Enterprise Routing\"\n",
    "    x1 = INPUT_SAGE1[INPUT_SAGE1['Email1'].str.contains(x, case=False)]\n",
    "    y1 = x1[x1['Technology Area Name'].str.contains(y, case=False)]\n",
    "    z = y1['Expertise Value Number'].max()\n",
    "    if z == 3:\n",
    "        return(\"In-Depth\")\n",
    "    elif z == 2:\n",
    "        return(\"Advanced\")\n",
    "    elif z == 1:\n",
    "        return(\"Foundational\")\n",
    "    else:\n",
    "        return(\"Unknown\")\n",
    "    \n",
    "def max_exp_sw(x):\n",
    "    y = \"Enterprise Switching\"\n",
    "    x1 = INPUT_SAGE1[INPUT_SAGE1['Email1'].str.contains(x, case=False)]\n",
    "    y1 = x1[x1['Technology Area Name'].str.contains(y, case=False)]\n",
    "    z = y1['Expertise Value Number'].max()\n",
    "    if z == 3:\n",
    "        return(\"In-Depth\")\n",
    "    elif z == 2:\n",
    "        return(\"Advanced\")\n",
    "    elif z == 1:\n",
    "        return(\"Foundational\")\n",
    "    else:\n",
    "        return(\"Unknown\")\n",
    "    \n",
    "def max_exp_wrl(x):\n",
    "    y = \"Wireless\"\n",
    "    x1 = INPUT_SAGE1[INPUT_SAGE1['Email1'].str.contains(x, case=False)]\n",
    "    y1 = x1[x1['Technology Area Name'].str.contains(y, case=False)]\n",
    "    z = y1['Expertise Value Number'].max()\n",
    "    if z == 3:\n",
    "        return(\"In-Depth\")\n",
    "    elif z == 2:\n",
    "        return(\"Advanced\")\n",
    "    elif z == 1:\n",
    "        return(\"Foundational\")\n",
    "    else:\n",
    "        return(\"Unknown\")\n",
    "    \n",
    "\n",
    "print(\"Loading Complete {0}\".format(datetime.datetime.now()))    \n",
    "print(\"DONE - {0}\".format(datetime.datetime.now()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Expertise from SAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STARTING - {0}\".format(datetime.datetime.now()))\n",
    "\n",
    "\n",
    "# Translate Expertise to Numbers\n",
    "INPUT_SAGE1['Expertise Value Number'] = INPUT_SAGE1['Expertise Value'].apply(set_expertise)\n",
    "\n",
    "\n",
    "INPUT_SAGE1['Routing Expertise'] = INPUT_SAGE1['Email1'].apply(max_exp_rtr)\n",
    "INPUT_SAGE1['Switching Expertise'] = INPUT_SAGE1['Email1'].apply(max_exp_sw)\n",
    "INPUT_SAGE1['Wireless Expertise'] = INPUT_SAGE1['Email1'].apply(max_exp_wrl)\n",
    "INPUT_SAGE1['uid'] = INPUT_SAGE1['Email1']\n",
    "\n",
    "INPUT_SAGE1a = INPUT_SAGE1.loc[:, ['uid', 'Routing Expertise', 'Switching Expertise', 'Wireless Expertise']]\n",
    "INPUT_SAGE1a.drop_duplicates(inplace=True)\n",
    "\n",
    "# Determine SEVT Membership\n",
    "email_key = INPUT_SAGE2.columns.values[0]\n",
    "INPUT_SAGE1a['EN SEVT Member'] = INPUT_SAGE1a['uid'].isin(INPUT_SAGE2[email_key])\n",
    "\n",
    "INPUT_DIR1a = INPUT_DIR1.loc[:,('uid', 'firstname', 'lastname', 'title', 'mgr', 'departmentdesc')]\n",
    "DIRECTORY = INPUT_DIR1a.merge(INPUT_DIR2, on='departmentdesc', how='left')\n",
    "  \n",
    "print(\"DONE - {0}\".format(datetime.datetime.now()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom directory mapping\n",
    "#### Add any one off modifications here\n",
    "#### IE:  The TSA's are in COMMERCIAL HQ, but we want to represent them in the Areas they support for reporting purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STARTING - {0}\".format(datetime.datetime.now()))\n",
    "\n",
    "def custom_dir(uid, new_value, col_name):\n",
    "    DIRECTORY[col_name][DIRECTORY['uid'] == uid] = new_value \n",
    "\n",
    "custom_dir('anbetz', 'COMMERCIAL EAST AREA', 'L4')\n",
    "custom_dir('bschnewe', 'COMMERCIAL CENTRAL AREA', 'L4')\n",
    "custom_dir('chijenki', 'COMMERCIAL SOUTH AREA', 'L4')\n",
    "custom_dir('dhighlan', 'COMMERCIAL WEST AREA', 'L4')\n",
    "custom_dir('dsheaffe', 'COMMERCIAL SOUTH AREA', 'L4')\n",
    "custom_dir('gwerner', 'COMMERCIAL WEST AREA', 'L4')\n",
    "custom_dir('sclayton', 'COMMERCIAL EAST AREA', 'L4')\n",
    "custom_dir('tyoutsey', 'COMMERCIAL CENTRAL AREA', 'L4')\n",
    "\n",
    "print(\"DONE - {0}\".format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Additional SmartSheet DataPoints\n",
    "#### IE:  SD-WAN POV Certification Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download fresh data from SmartSheets\n",
    "print(\"STARTING - {0}\".format(datetime.datetime.now()))\n",
    "\n",
    "# TODO: Update this with the ID of your sheet to update\n",
    "sheet_id = 3103471428757380 #SD-WAN POV Tracker\n",
    "download_path = \".\"\n",
    "\n",
    "# Call the download module\n",
    "response = ss.Sheets.get_sheet_as_excel(sheet_id, download_path)\n",
    "\n",
    "print(\"Downloaded '{0}' SmartSheet to '{1}'\".format(response.filename,response.download_directory))\n",
    "\n",
    "downloaded_ss = response.filename\n",
    "\n",
    "input_file_ss_02 = pd.ExcelFile(downloaded_ss)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "INPUT_POVCert_01 = input_file_ss_02.parse(input_file_ss_02.sheet_names[0])\n",
    "INPUT_POVCert_01['uid'] = INPUT_POVCert_01['Email Address'].apply(split_email)\n",
    "INPUT_POVCert_01['SD-WAN \"DLP\" POV Certified'] = INPUT_POVCert_01['Status']\n",
    "INPUT_POVCert_02 = INPUT_POVCert_01[['uid', 'SD-WAN \"DLP\" POV Certified']][INPUT_POVCert_01['uid'].isin(DIRECTORY['uid'])]\n",
    "\n",
    "print(\"DONE - {0}\".format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map UserID's to Directory Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STARTING - {0}\".format(datetime.datetime.now()))\n",
    "\n",
    "\n",
    "SAGE_DIR1 = INPUT_SAGE1a.merge(DIRECTORY, on='uid', how='inner')\n",
    "SAGE_DIR1 = SAGE_DIR1.merge(INPUT_POVCert_02, on='uid', how='inner')\n",
    "\n",
    "# Create email and directory links\n",
    "SAGE_DIR1['Email Address'] = SAGE_DIR1['uid'].apply(set_email)\n",
    "SAGE_DIR1['Directory'] = SAGE_DIR1['uid'].apply(set_dir)\n",
    "\n",
    "# Fill missing Region info\n",
    "SAGE_DIR1 = SAGE_DIR1.fillna('Unknown')\n",
    "OUTPUT1 = SAGE_DIR1[['L4', 'L5', 'L6', 'uid', 'firstname', 'lastname','title', 'mgr', 'Email Address', 'Directory', 'EN SEVT Member', 'SD-WAN \"DLP\" POV Certified', 'Routing Expertise','Switching Expertise', 'Wireless Expertise' ]]\n",
    "OUTPUT1.dropna(axis=0, how='any')\n",
    "OUTPUT2 = OUTPUT1.drop_duplicates(keep='first')\n",
    "print(\"Loaded local CSV files {0}\".format(datetime.datetime.now()))\n",
    "print(\"DONE - {0}\".format(datetime.datetime.now()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Full Updated Champions Report to Local XLSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STARTING - {0}\".format(datetime.datetime.now()))\n",
    "\n",
    "\n",
    "writer = pd.ExcelWriter(output_file)\n",
    "OUTPUT2.to_excel(writer,'EN Champions',index=False)\n",
    "writer.save()\n",
    "print(\"Saved {1} to disk {0}\".format(datetime.datetime.now(), 'EN Champions'))\n",
    "print(\"DONE - {0}\".format(datetime.datetime.now()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export SmartSheets Champion Sheet to local XLSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download fresh data from SmartSheets\n",
    "print(\"STARTING - {0}\".format(datetime.datetime.now()))\n",
    "\n",
    "# TODO: Update this with the ID of your sheet to update\n",
    "sheet_id = 809447254714244 #Prod EN Champions\n",
    "# sheet_id = 8459983764383620 #Dev EN Champions\n",
    "download_path = \".\"\n",
    "\n",
    "# Call the download module\n",
    "response = ss.Sheets.get_sheet_as_excel(sheet_id, download_path)\n",
    "\n",
    "print(\"Downloaded '{0}' SmartSheet to '{1}'\".format(response.filename,response.download_directory))\n",
    "\n",
    "downloaded_ss = response.filename\n",
    "print(\"DONE - {0}\".format(datetime.datetime.now()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine list of changes from SmartSheet to new Champions report\n",
    "### Save changes to local XLSX for manual reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STARTING - {0}\".format(datetime.datetime.now()))\n",
    "\n",
    "\n",
    "input_file04 = pd.ExcelFile(downloaded_ss)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "INPUT_OLD = input_file04.parse(input_file04.sheet_names[0])\n",
    "\n",
    "REMOVE_LIST = INPUT_OLD[~INPUT_OLD['uid'].isin(INPUT_DIR1['uid'])]\n",
    "ADD_LIST = OUTPUT2[~OUTPUT2['uid'].isin(INPUT_OLD['uid'])]\n",
    "KEEP_OLD = INPUT_OLD[INPUT_OLD['uid'].isin(INPUT_DIR1['uid'])]\n",
    "KEEP_NEW = OUTPUT2[OUTPUT2['uid'].isin(INPUT_OLD['uid'])]\n",
    "KEEP_OLD['EN SEVT Member'].fillna(0, inplace=True)\n",
    "\n",
    "KEEP_NEW_01 = KEEP_NEW[['L4', 'L5', 'L6', 'uid', 'firstname', 'lastname', 'title', 'mgr', 'Email Address', 'Directory', 'EN SEVT Member', 'SD-WAN \"DLP\" POV Certified', 'Routing Expertise', 'Switching Expertise', 'Wireless Expertise']].set_index('uid')\n",
    "KEEP_OLD_01 = KEEP_OLD[['L4', 'L5', 'L6', 'uid', 'firstname', 'lastname', 'title', 'mgr', 'Email Address', 'Directory', 'EN SEVT Member', 'SD-WAN \"DLP\" POV Certified',  'Routing Expertise', 'Switching Expertise', 'Wireless Expertise']].set_index('uid')\n",
    "\n",
    "KEEP_NEW_01['EN SEVT Member'] = KEEP_NEW_01['EN SEVT Member'].astype('bool')\n",
    "KEEP_OLD_01['EN SEVT Member'] = KEEP_OLD_01['EN SEVT Member'].astype('bool')\n",
    "\n",
    "CHANGE_LIST = pd.DataFrame()\n",
    "cols = KEEP_OLD_01.columns\n",
    "\n",
    "update_dict1 = {}\n",
    "\n",
    "total_records = 0\n",
    "update_records = 0\n",
    "\n",
    "for u in KEEP_NEW['uid']:\n",
    "    try:\n",
    "        total_records += 1\n",
    "        if (KEEP_OLD_01.loc[u, :] != KEEP_NEW_01.loc[u, :]).any():\n",
    "            CHANGE_LIST = CHANGE_LIST.append(KEEP_NEW_01.loc[u, :])\n",
    "            update_records += 1\n",
    "            changes = []        \n",
    "            for col in cols:\n",
    "                if KEEP_OLD_01.loc[u, col] != KEEP_NEW_01.loc[u, col]:\n",
    "                    changes.append(col)\n",
    "                    update_dict1[u] = {col : KEEP_NEW_01.loc[u, col]}\n",
    "                    #print(u, col, KEEP_NEW_01.loc[u, col])\n",
    "            CHANGE_LIST.loc[u, \"Changes\"] = str(changes)\n",
    "    except:\n",
    "        print(\"Could not process {0} - Review Manually\".format(u))\n",
    "        update_dict1[u] = {\"Update Needed\" : \"Error\"}\n",
    "\n",
    "        \n",
    "print(\"{0} of {1} records to update.\".format(update_records, total_records))\n",
    "    \n",
    "\n",
    "CHANGE_LIST['uid'] = CHANGE_LIST.index\n",
    "\n",
    "output_file = \"Changes.xlsx\"\n",
    "\n",
    "writer = pd.ExcelWriter(output_file)\n",
    "try:\n",
    "    CHANGE_LIST[['L4', 'L5', 'L6', 'uid', 'firstname', 'lastname', 'title', 'mgr', 'Email Address', 'Directory', 'EN SEVT Member', 'SD-WAN \"DLP\" POV Certified',  'Routing Expertise', 'Switching Expertise', 'Wireless Expertise', 'Changes']].to_excel(writer,'CHANGE_LIST',index=False)\n",
    "except:\n",
    "    print(\"Error writing CHANGE_LIST to file\")\n",
    "    \n",
    "try:\n",
    "    REMOVE_LIST[['L4', 'L5', 'L6', 'uid', 'firstname', 'lastname', 'title', 'mgr', 'Email Address', 'Directory', 'EN SEVT Member', 'SD-WAN \"DLP\" POV Certified',  'Routing Expertise', 'Switching Expertise', 'Wireless Expertise']].to_excel(writer,'REMOVE_LIST',index=False)\n",
    "except:\n",
    "    print(\"Error writing REMOVE_LIST to file\")\n",
    "    \n",
    "try:\n",
    "    ADD_LIST[['L4', 'L5', 'L6', 'uid', 'firstname', 'lastname', 'title', 'mgr', 'Email Address', 'Directory', 'EN SEVT Member', 'SD-WAN \"DLP\" POV Certified',  'Routing Expertise', 'Switching Expertise', 'Wireless Expertise']].to_excel(writer,'ADD_LIST',index=False)\n",
    "except:\n",
    "    print(\"Error writing ADD_LIST to file\")   \n",
    "\n",
    "writer.save()\n",
    "\n",
    "print(\"List of changes saved to {0} - {1}\".format(output_file, datetime.datetime.now()))\n",
    "\n",
    "print(\"DONE - {0}\".format(datetime.datetime.now()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STARTING - {0}\".format(datetime.datetime.now()))\n",
    "\n",
    "Col_Update = ss.Sheets.get_column_by_title(sheet_id, \"Update Needed\").id\n",
    "\n",
    "def update_needed(uid, text):\n",
    "    r = ss.Sheets.search_sheet(sheet_id, uid)\n",
    "    r2 = json.loads(str(r))\n",
    "    Row_Update = r2['results'][0]['objectId']\n",
    "\n",
    "    # Build new cell value\n",
    "    new_cell = ss.models.Cell()\n",
    "    new_cell.column_id = Col_Update\n",
    "    new_cell.value = str(text)\n",
    "    new_cell.strict = False\n",
    "\n",
    "    # Build the row to update\n",
    "    new_row = ss.models.Row()\n",
    "    new_row.id = Row_Update\n",
    "    new_row.cells.append(new_cell)\n",
    "\n",
    "    # Update rows\n",
    "    updated_row = ss.Sheets.update_rows(sheet_id, [new_row])\n",
    "    \n",
    "for u in REMOVE_LIST['uid']:\n",
    "    update_needed(u, \"REMOVE\")\n",
    "    \n",
    "\n",
    "column_names = get_column_names(sheet_id)\n",
    "\n",
    "update_cells_by_dict(update_dict1)\n",
    "    \n",
    "print(\"DONE - {0}\".format(datetime.datetime.now()))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
